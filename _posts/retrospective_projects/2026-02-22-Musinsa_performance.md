---
layout: post
title: "수강신청 시스템 성능 분석 — 락 vs 대기열"
date: 2026-02-22
categories: [retrospective]
description: "동기/비동기 Redis 구조적 한계와 대기열 시스템 도입 분석"
---

![무신사 이미지](/public/images/musinsa.png)

## 개요

> 과제가 종료된 후, 병목 지점을 검토했다.
>
> 사용자 → nginx → WAS → Redis | DB 흐름에서 병목이 발생하는 지점은 보통 두 곳이다.
>
> 1. `nginx → WAS`: Worker가 바빠서 발생
> 2. `WAS → Redis | DB`: I/O 처리 지연
>
> 1개의 Worker가 최대 몇 건의 요청을 처리할 수 있을까? AS-IS 상태에서 실측한 결과 초당 약 280건이었다. 비동기 Redis로 전환하면 더 많은 양을 처리할 수 있겠다는 생각에서 분석을 시작했고, 동기/비동기 Redis와 BLPOP의 조합에서 각각 다른 구조적 한계가 드러났다.

---

## 동기 / 비동기 Redis의 구조적 한계

**동기 Redis + BLPOP**

> 워커 N개 = 최대 커넥션 N개. BLPOP이 스레드를 블로킹하므로 커넥션이 터질 구조 자체가 없다.
>
> 단, BLPOP 대기 중에는 스레드 전체가 멈추므로 FastAPI의 async 장점을 완전히 포기하게 된다. 사실상 동기 서버로 동작한다.

**비동기 Redis + BLPOP**

> 코루틴은 수천 개 동시 실행이 가능하다. 각 코루틴이 BLPOP 대기 중 커넥션을 점유하므로, 18,000개 요청이 동시에 대기에 들어가면 18,000개의 커넥션이 필요해진다.
>
> 커넥션 풀 기본 크기(10~50)를 즉시 고갈시키고, 풀 크기를 늘리면 이번엔 Redis 서버의 `maxclients`(기본 10,000, 실무 상한 ~20,000)에 걸린다.
>
> FastAPI의 비동기 처리량을 살리려다가 오히려 Redis 커넥션 풀 고갈이라는 더 큰 문제가 생긴다.

> 어느 방향이든 **BLPOP을 잡고 대기하는 구조 자체가 문제**다. 이것이 대기열 시스템을 검토하게 된 출발점이다.

---

## AS-IS: Redis 분산 락

**흐름**

![sync 시퀸스 다이어그램](/public/images/sync.png)

```
사용자 요청 → BLPOP(락 대기) → SET owner → 비즈니스 로직 → Lua 스크립트 해제
```

**문제점**

> - 요청 1건당 Redis 연산 4~6회 (BLPOP, SET, Lua, DEL 등) → 18,000 RPS 기준 약 90,000 ops/sec로 단일 Redis 한계(~100,000 ops/sec)에 근접
> - 락 경쟁이 심해질수록 타임아웃으로 인한 실패율 급등 (워커를 늘려도 Non-2xx 95% 이상)
> - 동기 Redis이므로 BLPOP이 스레드를 블로킹 → FastAPI async 장점 미활용, 처리량이 워커 수에 직접 비례

---

## TO-BE: 대기열 시스템

> 대기열로 바꾸면 API 서버는 `ZADD` 1번만 하고 즉시 반환한다. 커넥션을 잡고 대기하는 일이 없으므로, 비동기 Redis를 사용해도 풀 고갈 문제가 없고 FastAPI async 장점도 온전히 활용할 수 있다.

```
AS-IS: 요청 → BLPOP 대기(커넥션 점유 최대 30초) → 처리
TO-BE: 요청 → ZADD(즉시 반환, 커넥션 반납)    → 202 응답
```

**흐름**

![async 시퀸스 다이어그램](/public/images/async.png)

```
사용자 요청 → ZADD(큐 삽입, 1회) → 즉시 202 + ticket_id 반환

[별도 Worker 프로세스]
ZPOPMIN → 정원 확인 → DB 쓰기 (단일 프로세스 직렬 처리)

클라이언트 폴링:
  GET /enrollments/queue/{ticket_id}/status
  ← { status: "pending" } → "처리 중"
  ← { status: "success" } → "신청 완료"
```

**다중 인스턴스에서의 Worker 분리**

> 추후 인스턴스를 추가하면 Worker도 여러 개가 되어 직렬화가 깨진다.

```
# 잘못된 구조
API Server A (Worker 포함) → ZPOPMIN → DB 쓰기
API Server B (Worker 포함) → ZPOPMIN → DB 쓰기
                                  ↑
                           두 Worker가 병렬로 DB 쓰기 → 정합성 문제 재발
```

> Worker를 단일 독립 서비스로 분리해야 직렬화가 보장된다.

```
[LB]
 ├── API Server A → ZADD만 (큐에 넣기)
 ├── API Server B → ZADD만 (큐에 넣기)
 │
 [공유 Redis ZSET]
 │
 └── Worker Server (단일 프로세스) → ZPOPMIN → DB 쓰기
```

---

## 부하 테스트 결과

> wrk 4t/100c/30s, Docker standalone 기준.
>
> 맥북 Air M2 (8코어)에서 wrk + Docker 등 타 프로세스가 동시 실행 중이었다. 4코어까지 선형적인 성능 향상이 이뤄지고 이후 수확 체감이 나타났기 때문에, 4 worker를 기준으로 측정했다.

| | 1w + 락 | 4w + 락 | 1w + 대기열(sync) | 4w + 대기열(sync) | 1w + 대기열(async) | 4w + 대기열(async) |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| RPS | 300 | 840 | 1,484 | 2,893 | 2,807 | **6,896** |
| Avg Latency | 339ms | 140ms | 68ms | 35ms | 36ms | **15ms** |
| Max Latency | 1,450ms | 826ms | 299ms | 164ms | 169ms | 84ms |

> async Redis + `async def` 엔드포인트로 전환하자 thread pool 한계가 사라지면서, 1 worker만으로 sync 4 worker 수준(2,807 vs 2,893)이 됐다. 4 worker 기준으로는 sync 대비 약 **2.4배**(6,896 vs 2,893) 향상됐다.

---

## 트레이드오프

| 관점 | AS-IS (락) | TO-BE (대기열) |
|---|---|---|
| 처리량 | 낮음 (락 경쟁 → 타임아웃) | 높음 (직렬화 → 실패 없음) |
| 응답 방식 | 동기 (즉시 성공/실패) | 비동기 (폴링 필요) |
| 실패 모드 | 단순 | 복잡 (Worker 장애, Redis OOM 대응 필요) |
| 인프라 복잡도 | 낮음 | 높음 (Worker 서비스 별도 운영) |
| UX | 즉시 결과 확인 가능 | 수 초간 "처리 중" 화면 노출 |

> 락과 대기열 중 하나가 절대적으로 우월한 것이 아니다. 처리량이 중요하다면 대기열, 구현 단순성과 즉각적인 UX가 중요하다면 락이 맞다. 서비스 특성과 팀의 운영 역량에 맞게 선택하는 것이 핵심이다.

---

## 결론

> 이번 과제를 통해 동시성 제어를 단계적으로 고도화하는 과정을 직접 경험했다.
>
> 처음에는 단순한 Redis Lock으로 시작했다. 이내 polling 방식이 FIFO를 보장하지 않는다는 걸 알게 됐고, BLPOP 기반 FIFO Lock으로 전환했다. 하지만 FIFO Lock을 구현하면서 원자성, Race Condition, 토큰 유실이라는 예상치 못한 문제들을 연달아 만났다. 각 문제를 하나씩 해결하는 과정에서 Redis의 동작 원리를 깊게 이해하게 됐다. 과제가 끝나고 나서는 동기/비동기 Redis의 구조적 한계를 분석하면서, BLPOP으로 대기하는 구조 자체가 문제라는 결론에 도달했다.

**배운 것**

> 설계가 엔지니어의 핵심 덕목이 될 것이라 생각한다.
>
> 병목 지점을 점검하고, 발생 가능한 시나리오를 구상한다. 해당 지점에서 발생할 문제를 미리 발견하고 예방한다.
>
> 이번 과제는 그 사고방식을 실제로 적용해본 경험이었다.
>
> 이 과제는 Claude Code를 활용해 개발했다. 직접 코드를 치는 시간보다 "왜 이렇게 동작하는가"를 이해하고 검증하는 데 더 많은 시간을 썼다. AI와 함께 개발할수록 설계 판단력이 더 중요해진다는 걸 체감했다.

**다음에 해보고 싶은 것**

> 비즈니스 로직이 단순하다보니 싱글 인스턴스로도 가능해보이지만, 실제 비즈니스 로직에서는 요청 처리량이 더 줄어들 것이다.
>
> 18,000개의 요청을 정말 다 소화해야 한다면 스케일업을 하거나, N개의 인스턴스로 스케일 아웃이 필요해보인다.
>
> N개의 인스턴스와 1개의 Worker 구성을 구현해보고 싶다.
