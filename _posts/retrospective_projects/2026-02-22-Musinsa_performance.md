---
layout: post
title: "수강신청 시스템 성능 분석 — 락 vs Celery Worker vs Celery Beat 배치"
date: 2026-02-22
categories: [retrospective]
description: "동기/비동기 Redis 구조적 한계, Celery Worker 대기열, Celery Beat 배치 처리 분석"
---

![무신사 이미지](/public/images/musinsa.png)

## 개요

> 과제가 종료된 후, 병목 지점을 검토했다.
>
> 사용자 → nginx → WAS → Redis | DB 흐름에서 병목이 발생하는 지점은 보통 두 곳이다.
>
> 1. `nginx → WAS`: Worker가 바빠서 발생
> 2. `WAS → Redis | DB`: I/O 처리 지연
>
> 1개의 Worker가 최대 몇 건의 요청을 처리할 수 있을까? AS-IS 상태에서 실측한 결과 초당 약 280건이었다. 대기열 시스템을 도입하면 더 많은 양을 처리할 수 있겠다는 생각에서 분석을 시작했고, 동기 Redis BLPOP 락 → Celery Worker 대기열 → Celery Beat 배치 처리로 단계적으로 고도화했다.

---

## 동기 / 비동기 Redis의 구조적 한계

**동기 Redis + BLPOP**

> 워커 N개 = 최대 커넥션 N개. BLPOP이 스레드를 블로킹하므로 커넥션이 터질 구조 자체가 없다.
>
> 단, BLPOP 대기 중에는 스레드 전체가 멈추므로 FastAPI의 async 장점을 완전히 포기하게 된다. 사실상 동기 서버로 동작한다.

**비동기 Redis + BLPOP**

> 코루틴은 수천 개 동시 실행이 가능하다. 각 코루틴이 BLPOP 대기 중 커넥션을 점유하므로, 18,000개 요청이 동시에 대기에 들어가면 18,000개의 커넥션이 필요해진다.
>
> 커넥션 풀 기본 크기(10~50)를 즉시 고갈시키고, 풀 크기를 늘리면 이번엔 Redis 서버의 `maxclients`(기본 10,000, 실무 상한 ~20,000)에 걸린다.
>
> FastAPI의 비동기 처리량을 살리려다가 오히려 Redis 커넥션 풀 고갈이라는 더 큰 문제가 생긴다.

> 어느 방향이든 **BLPOP을 잡고 대기하는 구조 자체가 문제**다. 이것이 대기열 시스템을 검토하게 된 출발점이다.

---

## AS-IS: Redis 분산 락

**흐름**

![sync 시퀸스 다이어그램](/public/images/sync.png)

```
사용자 요청 → BLPOP(락 대기) → SET owner → 비즈니스 로직 → Lua 스크립트 해제
```

**문제점**

> - 요청 1건당 Redis 연산 4~6회 (BLPOP, SET, Lua, DEL 등) → 18,000 RPS 기준 약 90,000 ops/sec로 단일 Redis 한계(~100,000 ops/sec)에 근접
> - 락 경쟁이 심해질수록 타임아웃으로 인한 실패율 급등 (워커를 늘려도 Non-2xx 95% 이상)
> - 동기 Redis이므로 BLPOP이 스레드를 블로킹 → FastAPI async 장점 미활용, 처리량이 워커 수에 직접 비례

---

## Celery Worker (단건 대기열)

> 대기열로 바꾸면 API 서버는 `ZADD` 1번만 하고 즉시 반환한다. 커넥션을 잡고 대기하는 일이 없으므로, 비동기 Redis를 사용해도 풀 고갈 문제가 없고 FastAPI async 장점도 온전히 활용할 수 있다.

```
락 방식: 요청 → BLPOP 대기(커넥션 점유 최대 30초) → 처리
Worker: 요청 → ZADD(즉시 반환, 커넥션 반납)    → 202 응답
```

**흐름**

![celery worker 시퀸스 다이어그램](/public/images/worker.png)

```
사용자 요청 → ZADD(큐 삽입, 1회) → 즉시 202 + ticket_id 반환

[Celery Worker — 50ms 폴링, 1건씩 처리]
  ZPOPMIN → 정원 확인 → 학점 확인 → 스케줄 충돌 확인 → DB 쓰기

클라이언트 폴링:
  GET /enrollments/queue/{ticket_id}/status
  ← { status: "pending" } → "처리 중"
  ← { status: "success" } → "신청 완료"
```

**API 레이어 처리량 (wrk 직접 측정)**

> wrk -t4 -c100 -d30s, Docker standalone (MacBook Air M2, 8코어).

| | 1w + 락 | 4w + 락 | 1w + Worker(sync) | 4w + Worker(sync) | 1w + Worker(async) | 4w + Worker(async) |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| RPS | 300 | 840 | 1,484 | 2,893 | 2,807 | **6,896** |
| Avg Latency | 339ms | 140ms | 68ms | 35ms | 36ms | **15ms** |
| Max Latency | 1,450ms | 826ms | 299ms | 164ms | 169ms | 84ms |

> async Redis + `async def` 엔드포인트 전환 후 thread pool 한계가 사라지면서, 1 worker만으로 sync 4 worker 수준(2,807 vs 2,893)에 도달했다. 4 worker 기준으로는 sync 대비 약 **2.4배**(6,896 vs 2,893) 향상됐다.

---

## 왜 Celery Worker를 사용하지 않았는가

> API 레이어 처리량은 확실히 개선됐다. 문제는 Worker 자체였다.

**Worker 처리 병목**

> Celery Worker는 큐에서 1건씩 꺼내 처리한다. 1건 처리 흐름은 다음과 같다.
>
> ```
> ZPOPMIN → DB 커넥션 획득 → 정원 확인 → 학점 확인 → 스케줄 충돌 확인 → DB 쓰기 → 커넥션 반납
> ```
>
> 정원 확인만 놓고 보면 Redis로 최적화할 여지가 있다. 강좌별 잔여 정원을 Redis에 미리 올려두고, `DECR`로 원자적으로 차감하면 DB를 보지 않아도 된다. Redis가 다운되더라도 Lua 스크립트로 DB에서 카운트를 복구하는 방식으로 내구성을 보완할 수 있다.
>
> 하지만 수강신청에는 정원 외에도 두 가지 검증이 더 필요하다.
>
> - **학점 제한**: 학생의 현재 누적 학점이 18학점을 초과하지 않는지
> - **시간표 충돌**: 기존에 신청한 강좌들과 시간이 겹치지 않는지
>
> 두 검증 모두 학생마다 다르고 신청이 들어올 때마다 값이 바뀐다. Redis에 미리 캐싱해두기 어렵고, 결국 요청 1건당 DB를 열어봐야 한다. 정원 카운트를 Redis로 옮겨도 학점·스케줄 검증에서 DB I/O가 발생하므로, Worker 단건 처리의 병목 구조 자체는 바뀌지 않는다.
>
> API 레이어가 초당 수천 건을 큐에 밀어 넣는 동안, Worker는 건당 DB I/O 비용을 모두 지불하며 1건씩 소화한다. 처리 속도보다 적재 속도가 훨씬 빠르므로 큐가 계속 쌓인다.

**DB 커넥션 고갈**

> 처리량을 높이려면 Worker 수를 늘려야 한다. 그런데 Worker N개 = DB 커넥션 N개 상시 점유다. 요청이 폭주하면 Worker가 일제히 DB 커넥션을 잡고, DB의 `max_connections` 한계에 빠르게 부딪힌다.

**순서 보장 문제**

> Worker 여러 개를 띄우면 직렬화가 깨진다. 취소 요청과 신청 요청이 서로 다른 Worker에 배분되면, 취소로 생긴 자리를 동일 시점의 신청이 활용할 수 없다.
>
> Worker 1개로 직렬화를 유지하면 처리량 한계에 묶이고, Worker 여러 개로 처리량을 높이면 순서 보장이 깨진다. 이 딜레마가 Celery Beat으로 전환한 핵심 이유다.

---

## TO-BE: Celery Beat 배치 처리

**최종 구조: Celery Beat 1초 배치 처리**

![celery beat 시퀸스 다이어그램](/public/images/beat.png)

```
사용자 요청 → ZADD(큐 삽입, 1회) → 즉시 202 + ticket_id 반환

[Celery Beat — 1초마다 실행]
  1. ZPOPMIN(취소 큐) → 취소 먼저 처리 (자리 복구)
  2. ZPOPMIN(신청 큐) → 복구된 자리 포함해 메모리 내 FIFO 검증
  3. 트랜잭션 1회: bulk DELETE + bulk INSERT + bulk UPDATE

클라이언트 폴링:
  GET /enrollments/queue/{ticket_id}/status
  ← { status: "pending" } → "처리 중"
  ← { status: "success" } → "신청 완료"
```

> 1초마다 한 번의 트랜잭션으로 전체 큐를 소진하는 구조는 **DB 쓰기 횟수를 극단적으로 줄인다**. 건당 1회 쓰기에서 배치당 1회 쓰기로 전환된다.
>
> 순서 보장도 구조적으로 해결된다. 1초마다 전체 큐를 한 번에 꺼내 **취소 → 신청 순서**로 메모리 내 처리하므로, 취소로 생긴 자리를 동일 배치 내 신청이 즉시 활용할 수 있다.


---

## Celery Beat 부하 테스트 결과

> wrk -t4 -c100 -d30s, Docker standalone (MacBook Air M2, 8코어), celery-beat 정상 실행 + Redis bgsave 수정 후 재측정.
>
> Beat 방식의 수치는 두 가지를 분리해서 봐야 한다. API 레이어(ZADD 수용 속도)와 실제 DB enrollment 처리량은 완전히 다른 개념이다.

**① API 레이어 처리량 (wrk 직접 측정)**

| | 1w + 락 | 4w + 락 | 1w + Beat | 4w + Beat |
|:---:|:---:|:---:|:---:|:---:|
| RPS | 282 | 828 | 2,990 | **7,490** |
| Avg Latency | 359ms | 134ms | 33ms | **14ms** |

**② 실제 DB enrollment 처리량 비교 — PERF_TEST_MODE (정원=9,999, 초기 수강신청 없음)**

> **[락 방식]** wrk 30s 직접 측정 후 DB 카운트 비교.\
> **[Beat 방식]** Beat 중지 → 46,899건 큐 적재 → Beat 재시작 후 드레인 완료까지 정밀 측정.

| | 1w + 락 | 4w + 락 | Beat (단일 worker) |
|:---:|:---:|:---:|:---:|
| 총 요청 처리 RPS | 272 | 775 | — |
| **실제 DB 등록/s** | **68.7** | **199.2** | **~3,658** |
| 큐 소화 속도 | — | — | ~47,000 items/s |
| 46,899건 소화 시간 | ~682s | ~235s | **~1초** |

> - Beat는 동일 46,899건을 **~1초**에 소화. 1w 락 기준 동일 처리에 ~682초 필요.
> - 실 등록 처리량: Beat 3,658/s vs 4w 락 199/s → **약 18배 차이**
> - Beat의 진짜 강점은 "락 없이 비동기 수용 → 배치 bulk insert 1회"로 DB 쓰기 횟수를 극단적으로 줄이는 것.

---

## 정합성 검증 (8가지 시나리오)

> 성능 수치만으로는 부족하다. 동시성 버그가 없는지 직접 검증했다.
> `bench/correctness_test.py`를 작성해 3가지 방식 × 4가지 시나리오를 커버했다.

| # | 시나리오 | 검증 기준 |
|---|---|---|
| 1 | 기본 정원 제한 — 분산 락 | `enrolled ≤ max_capacity` |
| 2 | 기본 정원 제한 — Celery Beat 배치 | `enrolled ≤ max_capacity` |
| 3 | 기본 정원 제한 — Celery 단건 | `enrolled ≤ max_capacity` |
| 4 | 수강취소 — Beat 배치 취소 큐 | 전건 취소 성공 |
| 5 | 취소·등록 동시 — Beat 배치 | 취소된 자리 재활용, `enrolled ≤ max_capacity` |
| 6 | 대규모 동시 요청 — Beat 배치 (100건+) | `enrolled ≤ max_capacity` |
| 7 | 학점 초과 동시 신청 — 분산 락 / Beat 배치 | `total_credits ≤ 18` |
| 8 | 시간표 충돌 동시 신청 — 분산 락 / Beat 배치 | 최종 시간표 충돌 없음 |

**시나리오 5 (취소·등록 동시) — Beat 배치 핵심**

> Beat는 동일 배치 내에서 **취소 먼저 → 신청 나중** 순서로 처리한다. 취소로 생긴 자리가 같은 배치 내 신청 요청에 즉시 활용된다.
>
> Celery Worker에서 Celery Beat으로 전환한 이유는 순서 보장만이 아니다. Worker 자체의 처리 병목 + DB 쓰기 횟수 절감이라는 실용적 판단이 함께 작용했다. 오버 엔지니어링처럼 보일 수 있지만, 배치당 트랜잭션 1회라는 구조가 가져오는 비용 절감은 명확하다.

```
처리 전: 37명 수강 (정원 100)
취소 8건 + 신청 18건 동시 제출
Beat 배치: 취소 8건 처리 → 자리 복구 → 신청 18건 성공
처리 후: 47명 수강 ✅ (정원 초과 없음)
```

## 트레이드오프

| 관점 | AS-IS (락) | TO-BE (Celery Beat 배치) |
|---|---|---|
| 처리량 | 낮음 (락 경쟁 → 타임아웃) | 높음 (직렬화 → 실패 없음) |
| 응답 방식 | 동기 (즉시 성공/실패) | 비동기 (폴링 필요, ~1초 대기) |
| 배치 내 순서 | 해당 없음 | 취소 → 신청 순서 보장 |
| 실패 모드 | 단순 | 복잡 (Celery Beat 장애) |
| 인프라 복잡도 | 낮음 | 높음 (Celery Beat 별도 운영) |
| UX | 즉시 결과 확인 가능 | 수 초간 "처리 중" 화면 노출 |

> 락과 Beat 중 하나가 절대적으로 우월한 것이 아니다. 처리량이 중요하다면 Beat, 구현 단순성과 즉각적인 UX가 중요하다면 락이 맞다. 서비스 특성과 팀의 운영 역량에 맞게 선택하는 것이 핵심이다.

---

## 결론

> 이번 과제를 통해 동시성 제어를 단계적으로 고도화하는 과정을 직접 경험했다.
>
> 처음에는 단순한 Redis Lock으로 시작했다. 이내 polling 방식이 FIFO를 보장하지 않는다는 걸 알게 됐고, BLPOP 기반 FIFO Lock으로 전환했다. 하지만 FIFO Lock을 구현하면서 원자성, Race Condition, 토큰 유실이라는 예상치 못한 문제들을 연달아 만났다. 각 문제를 하나씩 해결하는 과정에서 Redis의 동작 원리를 깊게 이해하게 됐다.
>
> 과제가 끝나고 나서는 동기/비동기 Redis의 구조적 한계를 분석하면서, BLPOP으로 대기하는 구조 자체가 문제라는 결론에 도달했다. 대기열 시스템을 도입했고, asyncio Worker를 먼저 검토했지만 Worker 자체에도 처리 병목이 생겼다. Celery Beat 배치 처리로 전환하면서 Worker 병목을 해소하고, DB 쓰기 횟수도 배치당 1회로 줄였다. 오버 엔지니어링처럼 보일 수 있지만 비용 절감 효과가 명확하고, "취소와 신청이 동시에 들어왔을 때 취소된 자리를 즉시 재활용"이라는 요구사항도 자연스럽게 충족됐다.

**배운 것**

> 설계가 엔지니어의 핵심 덕목이 될 것이라 생각한다.
>
> 병목 지점을 점검하고, 발생 가능한 시나리오를 구상한다. 해당 지점에서 발생할 문제를 미리 발견하고 예방한다.
>
> 이번 과제는 그 사고방식을 실제로 적용해본 경험이었다.
>
> 이 과제는 Claude cli를 활용해 개발했다. 직접 코드를 치는 시간보다 "왜 이렇게 동작하는가"를 이해하고 검증하는 데 더 많은 시간을 썼다. AI와 함께 개발할수록 설계 판단력이 더 중요해진다는 걸 체감했다.

**다음에 해보고 싶은 것**

> 비즈니스 로직이 단순하다 보니 Celery Beat 단일 인스턴스로도 가능해 보이지만, 실제 비즈니스 로직에서는 요청 처리량이 더 줄어들 것이다.
>
> 18,000개의 요청을 정말 다 소화해야 한다면 스케일업을 하거나, N개의 인스턴스로 스케일 아웃이 필요해 보인다.
>
> 일반적인 티켓팅 시스템에서는 Worker를 사용한다고 한다. 티켓팅은 `콘서트:좌석번호` 형태로 Redis에 미리 적재하고, 좌석 단위로 Redis lock을 건다. 좌석이 많아야 30,000개 수준이라 키 수도 제한적이고, 학점·스케줄 같은 학생별 DB 검증이 없어 Redis 레벨에서 거의 다 처리할 수 있다. DB I/O가 최소화되니 Worker가 건당 처리해도 병목이 생기지 않는 구조다.
>
> 티켓팅 시스템을 만들어 N개의 API 인스턴스 + N개의 Celery Worker 구성을 구현해보고 싶다. 이 구조에서 Worker 장애 시 큐가 쌓이는 문제와 재처리(idempotency) 보장도 함께 다뤄보고 싶다.
